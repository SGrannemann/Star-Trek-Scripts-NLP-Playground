{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python386jvsc74a57bd059507d4b7a424780c7daeb44aadd3841b613d4d87d8fb5cbad99945b5c4a9b7a",
   "display_name": "Python 3.8.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "def remove_speakers_and_empty_lines(episode_content: str) -> str:\n",
    "    \"\"\"Removes superfluous empty lines and the names of the speakers from the input data:\n",
    "    e.g. :\n",
    "    Picard: Make it so.\n",
    "    becomes:\n",
    "    Make it so.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in episode_content.split('\\n'):\n",
    "        # ignore empty lines\n",
    "        if line == '':\n",
    "            continue\n",
    "        # lines that start with square brackets are just information about the location.\n",
    "        if line.startswith('['):\n",
    "            continue\n",
    "        # the actual talking lines always contain a ':' - we will just keep the text, not the talker\n",
    "        # for this application\n",
    "        if ':' in line:\n",
    "            part_to_keep = line.split(':')[1]\n",
    "            cleaned_lines.append(part_to_keep.strip() + ' \\n')\n",
    "            continue\n",
    "        # after this string there are only information about the franchise, we can leave those out.\n",
    "        if line == '<Back':\n",
    "            break\n",
    "        \n",
    "        \n",
    "        cleaned_lines.append(line.strip() + ' \\n')\n",
    "    return ''.join(cleaned_lines)\n",
    "\n",
    "\n",
    "all_series_scripts = pd.read_json('all_scripts_raw.json')\n",
    "#  remove the names of the speakers and get rid of the empty lines\n",
    "# and I'll focus on The Next Generation Episodes for now\n",
    "tng_series_scripts_cleaned = all_series_scripts.TNG.map(remove_speakers_and_empty_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# after this preprocessing we want to check for outliers in the data\n",
    "# thinks like miss spelled words etc.\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# lets make a single text from all episodes.\n",
    "text = ''\n",
    "for index, episode_text in tng_series_scripts_cleaned.items():\n",
    "    text += episode_text\n",
    "\n",
    "\n",
    "# tokenize the text\n",
    "tokenized_text =  word_tokenize(text)\n",
    "\n",
    "fd = FreqDist(tokenized_text)\n",
    "\n",
    "print(len(fd))\n",
    "print(len(fd.hapaxes()))\n",
    "# as we can see here, our corpus contains 26404 different words - though we did not do any normalization like casefolding, stemming or lemmatization, so the actual different words will be less\n",
    "# whats even more astounding for me is number of words that occur only once: almost half of all words!\n",
    "# this approach apparently isnt helpful in finding missspelled words\n",
    "# but lets see if we can glimpse whether those are misspelled words\n",
    "print(fd.hapaxes()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# so, there are almost no spelling mistakes - that is good news! this should allow us to continue for now.\n",
    "\n",
    "# now we can have a look whether it makes sense to combine words into combined tokens based on their cooccurence.\n",
    "from gensim.models.phrases import Phrases\n",
    "from nltk.tokenize import sent_tokenize\n",
    "# gensims Phrases class expects a sequence of sentences, where each sentence is a list of tokens\n",
    "sentences = sent_tokenize(text)\n",
    "word_tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "bigrams = Phrases(word_tokenized_sentences)\n",
    "\n",
    "# lets test that on the first episode\n",
    "first_episode_bigrams = bigrams[word_tokenize(tng_series_scripts_cleaned[0])]\n",
    "print(first_episode_bigrams)\n",
    "\n",
    "# now that looks good - not too many bigrams, but a few typical combinations like tractor beam have been found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}